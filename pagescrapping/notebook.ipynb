{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pagescrapping.wayland import WaylandGamesScraper\n",
    "import asyncio\n",
    "\n",
    "scraper = WaylandGamesScraper()\n",
    "\n",
    "# Set the actual URL\n",
    "start_url = \"https://www.waylandgames.co.uk\"  # Replace with the real URL of the target page\n",
    "\n",
    "# Run the scraper asynchronously\n",
    "asyncio.run(scraper.run(start_url))\n",
    "\n",
    "# Save the data to a JSON file\n",
    "scraper.save_data('scraped_data.json')\n",
    "\n",
    "# Optionally, display the scraped data\n",
    "scraped_data = scraper.get_data()\n",
    "scraped_data  # This will display the scraped data in the notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin\n",
    "from playwright.async_api import async_playwright\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import json\n",
    "import logging\n",
    "\n",
    "class WebScraper:\n",
    "    def __init__(self, start_url, link_selector, subsequent_link_selector, max_initial_links):\n",
    "        self.start_url = start_url\n",
    "        self.link_selector = link_selector  \n",
    "        self.subsequent_link_selector = subsequent_link_selector  \n",
    "        self.max_initial_links = max_initial_links\n",
    "        self.visited_urls = set()\n",
    "        self.final_links = set()\n",
    "        self.stack = []  # Stack to manage DFS\n",
    "\n",
    "    async def scrape(self):\n",
    "        async with async_playwright() as p:\n",
    "            browser = await p.chromium.launch(headless=True)\n",
    "            page = await browser.new_page()\n",
    "            try:\n",
    "                await self._scrape_page(page, self.start_url)  # Start scraping from the main page\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during scraping: {e}\")\n",
    "            finally:\n",
    "                await browser.close()\n",
    "                self.save_to_json('tmp/final_links.json')\n",
    "\n",
    "    async def _scrape_page(self, page, url):\n",
    "        self.stack.append(url)\n",
    "        initial_links_count = 0\n",
    "        is_first_page = True\n",
    "        \n",
    "        while self.stack:\n",
    "            current_url = self.stack.pop()\n",
    "\n",
    "            if current_url in self.visited_urls:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                logging.info(f\"Navigating to {current_url}\")\n",
    "                await page.goto(current_url, timeout=60000)\n",
    "\n",
    "                if is_first_page:\n",
    "                    logging.info(f\"Waiting for initial selector {self.link_selector}\")\n",
    "                    await page.wait_for_selector(self.link_selector, timeout=20000)\n",
    "                    \n",
    "                    \n",
    "                    links = await page.query_selector_all(f\"{self.link_selector} a\")\n",
    "                    logging.info(f\"Found {len(links)} links on {current_url}\")\n",
    "\n",
    "                    \n",
    "                    links = links[:self.max_initial_links]\n",
    "                    is_first_page = False  \n",
    "\n",
    "                    for link in links:\n",
    "                        href = await link.get_attribute('href')\n",
    "                        if href:\n",
    "                            full_url = urljoin(self.start_url, href)\n",
    "                            if full_url not in self.visited_urls:\n",
    "                                self.stack.append(full_url)\n",
    "                                logging.info(f\"Added initial link to stack: {full_url}\")\n",
    "                else:\n",
    "                    logging.info(f\"Waiting for subsequent selector {self.subsequent_link_selector}\")\n",
    "                    await page.wait_for_selector(self.subsequent_link_selector, timeout=20000)\n",
    "\n",
    "                    links = await page.query_selector_all(f\"{self.subsequent_link_selector} a\")\n",
    "                    logging.info(f\"Found {len(links)} subsequent links on {current_url}\")\n",
    "\n",
    "                    if not links:\n",
    "                        logging.info(f\"No more links on {current_url}, adding to final links\")\n",
    "                        self.final_links.add(current_url)\n",
    "                        self.visited_urls.add(current_url)\n",
    "                    else:\n",
    "                        for link in links:\n",
    "                            href = await link.get_attribute('href')\n",
    "                            if href:\n",
    "                                full_url = urljoin(self.start_url, href)\n",
    "                                if full_url not in self.visited_urls:\n",
    "                                    self.final_links.add(full_url)\n",
    "                                    logging.info(f\"retrieved link: {full_url}\")\n",
    "\n",
    "                logging.info(f\"cxurrent stack size: {len(self.stack)}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"an error occurred while scraping {current_url}: {e}\")\n",
    "\n",
    "    def save_to_json(self, filename):\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(list(self.final_links), f, ensure_ascii=False, indent=4)\n",
    "        logging.info(f\"Data saved to {filename}\")\n",
    "\n",
    "start_url = \"https://elementgames.co.uk/\"\n",
    "link_selector = \"body > nav > div:nth-child(3) > div > div > div > ul\" \n",
    "subsequent_link_selector = \"body > div.section-colored.text-center > div > div:nth-child(2)\"\n",
    "max_initial_links = 1 \n",
    "\n",
    "scraper = WebScraper(start_url, link_selector, subsequent_link_selector, max_initial_links)\n",
    "asyncio.run(scraper.scrape())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "import logging\n",
    "from read import pd_to_file, pd_read_file\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "async def retrieve_product_info(json_file, output_file):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        \n",
    "        # Load the URLs from the JSON file\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            urls = json.load(f)\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for url in urls:\n",
    "            retries = 3\n",
    "            while retries > 0:\n",
    "                try:\n",
    "                    logging.info(f\"Navigating to {url}\")\n",
    "                    await page.goto(url, timeout=20000)\n",
    "\n",
    "    \n",
    "                    title_element = await page.query_selector(\"body > div.section-colored.text-center > div > div.product-wrap.row > div.product-info.col-xs-12.col-sm-12.col-md-7.col-lg-7 > div > div > h1\")\n",
    "                    title = await title_element.inner_text() if title_element else \"Title not found\"\n",
    "\n",
    "                   \n",
    "                    price_element = await page.query_selector(\"#testproduct > div > div.price-wrap > span.currentPrice\")\n",
    "                    price = await price_element.inner_text() if price_element else \"price not found\"\n",
    "                    results.append({\"url\": url, \"product_title\": title, \"price\": price})\n",
    "                    logging.info(f\"Retrieved: {title} - {price}\")\n",
    "                    break\n",
    "\n",
    "                except Exception as e:\n",
    "                    retries -= 1\n",
    "                    logging.error(f\"Error retrieving information from {url}: {e}. Retries left: {retries}\")\n",
    "                    if retries == 0:\n",
    "                        results.append({\"url\": url, \"product_title\": \"Error\", \"price\": \"Error\"})\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "        df = pd.DataFrame(results)\n",
    "        pd_to_file(df, output_file, index=False)\n",
    "        logging.info(f\"Data saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    json_input_file = 'tmp/elementalgame.json'\n",
    "    output_csv_file = 'tmp/elementgame.csv'\n",
    "    asyncio.run(retrieve_product_info(json_input_file, output_csv_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read import pd_to_file, pd_read_file\n",
    "df = pd_read_file('tmp/elementgame.csv', sep=\"\\t\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 119\u001b[0m\n\u001b[1;32m    116\u001b[0m max_links \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Maximum number of links to scrape per layer\u001b[39;00m\n\u001b[1;32m    118\u001b[0m scraper \u001b[38;5;241m=\u001b[39m WebScraper(start_url, sel1, sel2, sel3, max_links)\n\u001b[0;32m--> 119\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/asyncio/runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m coroutines\u001b[38;5;241m.\u001b[39miscoroutine(main):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma coroutine was expected, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(main))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "######################argos \n",
    "from urllib.parse import urljoin\n",
    "from playwright.async_api import async_playwright\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import json\n",
    "import logging\n",
    "\n",
    "class WebScraper:\n",
    "    def __init__(self, start_url, sel1, sel2, sel3, max_links):\n",
    "        self.start_url = start_url\n",
    "        self.sel1 = sel1  # First selector\n",
    "        self.sel2 = sel2  # Second selector\n",
    "        self.sel3 = sel3  # Third selector\n",
    "        self.max_links = max_links\n",
    "        self.visited = set()\n",
    "        self.final_links = set()\n",
    "        self.stack = []  # Stack to manage DFS\n",
    "\n",
    "    async def scrape(self):\n",
    "        async with async_playwright() as p:\n",
    "            browser = await p.chromium.launch(headless=True)\n",
    "            page = await browser.new_page()\n",
    "            try:\n",
    "                await self._scrape_page(page, self.start_url)  # Start from the main page\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Scrape error: {e}\")\n",
    "            finally:\n",
    "                await browser.close()\n",
    "                self.save_to_json('tmp/final_links.json')\n",
    "\n",
    "    async def _scrape_page(self, page, url):\n",
    "        self.stack.append(url)\n",
    "        is_first_page = True\n",
    "        \n",
    "        while self.stack:\n",
    "            curr_url = self.stack.pop()\n",
    "\n",
    "            if curr_url in self.visited:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                logging.info(f\"Navigating to {curr_url}\")\n",
    "                await page.goto(curr_url, timeout=60000)\n",
    "\n",
    "                # For the first page\n",
    "                if is_first_page:\n",
    "                    logging.info(f\"Waiting for initial selector {self.sel1}\")\n",
    "                    await page.wait_for_selector(self.sel1, timeout=60000)\n",
    "                    \n",
    "                    links = await page.query_selector_all(f\"{self.sel1} a\")\n",
    "                    logging.info(f\"Found {len(links)} links on {curr_url}\")\n",
    "                    \n",
    "                    links = links[:self.max_links]  # Limit initial links\n",
    "                    is_first_page = False  # Toggle off first page check\n",
    "                    \n",
    "                    for link in links:\n",
    "                        href = await link.get_attribute('href')\n",
    "                        if href:\n",
    "                            full_url = urljoin(self.start_url, href)\n",
    "                            if full_url not in self.visited:\n",
    "                                self.stack.append(full_url)\n",
    "                                logging.info(f\"Added link to stack: {full_url}\")\n",
    "                \n",
    "                # For the second page (based on sel2)\n",
    "                else:\n",
    "                    logging.info(f\"Waiting for second selector {self.sel2}\")\n",
    "                    await page.wait_for_selector(self.sel2, timeout=20000)\n",
    "\n",
    "                    links = await page.query_selector_all(f\"{self.sel2} a\")\n",
    "                    logging.info(f\"Found {len(links)} subsequent links on {curr_url}\")\n",
    "\n",
    "                    for link in links:\n",
    "                        href = await link.get_attribute('href')\n",
    "                        if href:\n",
    "                            full_url = urljoin(self.start_url, href)\n",
    "                            if full_url not in self.visited:\n",
    "                                logging.info(f\"Clicking through second-level link: {full_url}\")\n",
    "                                await self._scrape_deeper(page, full_url)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Scrape error at {curr_url}: {e}\")\n",
    "\n",
    "    async def _scrape_deeper(self, page, url):\n",
    "        try:\n",
    "            logging.info(f\"Navigating deeper to {url}\")\n",
    "            await page.goto(url, timeout=60000)\n",
    "            \n",
    "            logging.info(f\"Waiting for third selector {self.sel3}\")\n",
    "            await page.wait_for_selector(self.sel3, timeout=20000)\n",
    "            \n",
    "            links = await page.query_selector_all(f\"{self.sel3} a\")\n",
    "            logging.info(f\"Found {len(links)} final-layer links on {url}\")\n",
    "\n",
    "            for link in links:\n",
    "                href = await link.get_attribute('href')\n",
    "                if href:\n",
    "                    full_url = urljoin(self.start_url, href)\n",
    "                    if full_url not in self.visited:\n",
    "                        self.final_links.add(full_url)\n",
    "                        logging.info(f\"Final link added: {full_url}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in deep scrape at {url}: {e}\")\n",
    "\n",
    "    def save_to_json(self, filename):\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(list(self.final_links), f, ensure_ascii=False, indent=4)\n",
    "        logging.info(f\"Data saved to {filename}\")\n",
    "\n",
    "# Usage\n",
    "start_url = \"https://www.argos.co.uk/\"\n",
    "sel1 = \"#main-content > section.md\\:ds-mx-\\[calc\\(50\\%-49vw\\)\\] > div > div.M052styles__Container-sc-1cubg5c-2.leIYqs > div > div\"  # First layer selector\n",
    "sel2 = \"#findability > div.browse > div > div.xs-12.lg-9 > div.browse__content-area > div:nth-child(4) > div\"  # Second layer selector\n",
    "sel3 = \"#findability > div.search > div > div.styles__Container-sc-1h5mbdb-0.bCQCYQ.xs-12--none.lg-9--none.xs-stack > div:nth-child(8) > div > div\"  # Third layer selector (modify according to the site structure)\n",
    "max_links = 1  # Maximum number of links to scrape per layer\n",
    "\n",
    "scraper = WebScraper(start_url, sel1, sel2, sel3, max_links)\n",
    "asyncio.run(scraper.scrape())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
